// helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ 
// helm repo update    
// helm install my-confluent confluentinc/cp-helm-charts

# helm 설치
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

# helm repo add & helm install
helm repo add incubator https://charts.helm.sh/incubator
wget https://raw.githubusercontent.com/helm/charts/master/incubator/kafka/values.yaml
mv values.yaml kafka-values.yaml

# kubectl get storageclass

helm install kafka -f kafka-values.yaml incubator/kafka

# helm 올라간거 지우는 부분
# helm list
# helm uninstall kafka

# kafka client 실행
wget https://raw.githubusercontent.com/BigdataFlatformDynamicConfiguration/kafka/main/kafka-client.yaml
kubectl apply -f kafka-client.yaml 

# kubectl delete -f kafka-client.yaml

git clone https://github.com/confluentinc/cp-helm-charts.git
cd cp-helm-charts/charts/cp-schema-registry
cp values.yaml ../schema-registry-values.yaml

//schema-registry-values.yaml 파일을 열어서 bootstrapServers: “” 부분에 kafka-headless:9092로 추가합니다.

helm install my-schema -f schema-registry-values.yaml cp-schema-registry

cd cp-helm-charts/charts/cp-kafka-connect
cp values.yaml ../my-connect-values.yaml
// my-connect-values.yaml 파일을 열어서 bootstrapServers: “” 부분에 kafka-headless:9092로 추가하고, url 부분에 schema-registry url 정보를 추가합니다.
// schema-registry 정보 확인방법 kubectl get svc

helm install my-connect -f my-connect-values.yaml cp-kafka-connect

kubectl apply -f kafka-client.yaml 
helm repo add stable https://charts.helm.sh/stable
helm repo update

https://github.com/helm/charts.git
cd charts
// helm install hadoop $(stable/hadoop/tools/calc_resources.sh 50) stable/hadoop 
// The optional calc_resources.sh script is used as a convenience helper to set the yarn.numNodes, and yarn.nodeManager.resources appropriately to utilize all nodes in the Kubernetes cluster and a given percentage of their resources. 
//For example, with a 3 node n1-standard-4 GKE cluster and an argument of 50, this would create 3 NodeManager pods claiming 2 cores and 7.5Gi of memory.
// https://github.com/helm/charts/tree/master/stable/hadoop
helm install hadoop stable/hadoop

kubectl exec -it kafka-client -- \
curl -X POST  http://my-connect-cp-kafka-connect:8083/connectors -H "Content-Type: application/json" -d '
{
    "name": "hdfs3-parquet-field",
    "config": {
        "connector.class": "io.confluent.connect.hdfs3.Hdfs3SinkConnector",
        "tasks.max": "1",
        "topics": "parquet_field_hdfs",
        "hdfs.url": "hdfs://localhost:9000",
        "flush.size": "3",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "value.converter": "io.confluent.connect.avro.AvroConverter",
        "value.converter.schema.registry.url":"http://localhost:8081",
        "confluent.topic.bootstrap.servers": "localhost:9092",
        "confluent.topic.replication.factor": "1",

        "format.class":"io.confluent.connect.hdfs3.parquet.ParquetFormat",
        "partitioner.class":"io.confluent.connect.storage.partitioner.FieldPartitioner",
        "partition.field.name":"is_customer"
    }
}
'

